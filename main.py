# -*- coding: utf-8 -*-
"""reto3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1O-MkdKO0R0L5skNCOS-VrKrmly4Z1a3r

#-[0]. {Optional} Load google drive for csv
"""

#from google.colab import drive
#drive.mount('/content/drive')

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/drive/My Drive/URJC/Cours/'Reconocimiento patrones'

#!ls

"""#-[1]. Importing package needed

"""

# Commented out IPython magic to ensure Python compatibility.
import sklearn
import numpy as np
import pandas as pd
import seaborn as sb

# %matplotlib inline
from matplotlib import pyplot as plt

# seed = 1234 #<- random generator seed (0 to get randomness)
seed = 0

"""#-[2]. Load data from CSV : X_all_set (features) and Y_all_set (labels)"""

X_all_set = pd.read_csv('Datasets/reto3_trainX.csv')
Y_all_set = pd.read_csv('Datasets/reto3_trainY.csv')

print("Features shape : " + str(X_all_set.shape))
print("Label shape : " + str(Y_all_set.shape))

pd.set_option("display.max_columns", None)
X_all_set.head()

Y_all_set.head()

names = X_all_set.columns
for i in names:
  print(i, end='   ')

Y_all_set.value_counts()

"""#-[3]. Split X_all_set and y_all_set into TRAIN and TEST in a single Stratified split

### Parameters

X is the dataframe with examples (rows) and attributes (columns)

Y is the dataframe with labels

test_size is the percentage of X separated; default is 0.2

random_state is a seed for pseudorandom generation (0 to get randomness)

### Return

X_train, Y_train = dataframes of (1-test_size)% of the X and Y

X_test, Y_test = dataframes of test_size% of the X and Y
"""

def single_stratified_split(X,Y,test_size=.2, random_state=0):
    from sklearn.model_selection import StratifiedShuffleSplit
    if random_state != 0:
      splitter = StratifiedShuffleSplit(n_splits=1, test_size=test_size, random_state=random_state)
    else:
      splitter = StratifiedShuffleSplit(n_splits=1, test_size=test_size)
    split_ix = splitter.split(X,Y)
    for train_ix, test_ix in split_ix:
        X_train = X.loc[train_ix].reset_index(drop=True)
        Y_train = Y.loc[train_ix].reset_index(drop=True)
        X_test  = X.loc[test_ix].reset_index(drop=True)
        Y_test  = Y.loc[test_ix].reset_index(drop=True)
    return X_train, Y_train, X_test, Y_test

"""### Spliting the dataset"""

valid_size = 0.2
X_train, Y_train, X_valid, Y_valid = \
   single_stratified_split( X_all_set, Y_all_set, test_size=valid_size, random_state=seed)

"""#-[4]. Fit a scaler to [0,1]"""

X_train.head()

print("Features train set shape : " + str(X_train.shape))

a = 0
for i in X_train['altitud']:
  print(i)
  a += 1
  if a >= 10:
    break

from sklearn.preprocessing import MinMaxScaler

scaler = MinMaxScaler()

X_train_scale = scaler.fit_transform(X_train)
print("Features train set shape (scaled) : " + str(X_train_scale.shape))

print(X_train_scale)

# Verify scale is good

fig, axes = plt.subplots(1,4)
axes[0].scatter(X_train['altitud'], X_train['azimut'], X_train['inclinacion'], X_train['DH_agua'])
axes[0].set_title("Original 1")
axes[1].scatter(X_train['DV_agua'], X_train['sombra_3pm'], X_train['Reserva_4'], X_train['reserva_2'])
axes[1].set_title("Original 2")
axes[2].scatter(X_train_scale[:, 0], X_train_scale[:, 1], X_train_scale[:, 2], X_train_scale[:, 3])
axes[2].set_title("MinMax 1")
axes[3].scatter(X_train_scale[:, 4], X_train_scale[:, 8], X_train_scale[:, 13], X_train_scale[:, 11])
axes[3].set_title("MinMax 2")
print("Checking if the scaling as been well done")
plt.show()

"""#-[5]. Reduction with PCA"""

from sklearn.decomposition import PCA

n_components = 10
# n_components = 0.5 
if n_components <= X_train_scale.shape[1]:
    pca = PCA(n_components = n_components)
    pca.fit(X_train_scale)
    X_proy = pca.transform(X_train_scale)
else:
    print("ERROR: the number of princial components has to be less or equal than data dimension !")

print(X_proy)

num_of_pc = len(pca.singular_values_)

strTitle = '\n First %d principal components' %(num_of_pc)
print(strTitle)
print(pca.components_.T)
strTitle = '\n First %d singular values ' %(num_of_pc)
print(strTitle)
print(pca.singular_values_.T)
strTitle = '\n Explained Variance Ratio'
print(strTitle)
print(pca.explained_variance_ratio_.T)
strTitle = '\n Accumulated Explained Variance Ratio'
print(strTitle)
print(np.cumsum(pca.explained_variance_ratio_.T))

"""#-[6]. Learning with GradientBoosting ensemble

### Taking 10-20 min with google colaboratory
"""

from sklearn.ensemble import GradientBoostingClassifier

max_depth = 20
n_estimators = 100
learning_rate= 0.2

gb_clf = GradientBoostingClassifier(max_depth=max_depth,        \
                                    n_estimators=n_estimators,  \
                                    learning_rate=learning_rate)

gb_clf.fit(X_train_scale, Y_train)

"""#-[7]. Testing the model with test dataset"""

from sklearn.metrics import accuracy_score
from sklearn.metrics import confusion_matrix, classification_report

Y_pred = gb_clf.predict(scaler.transform(X_valid))
cm = confusion_matrix(Y_valid, Y_pred)
cr = classification_report(Y_valid, Y_pred)

accuracy = accuracy_score(Y_valid, Y_pred)

print("accuracy = ",accuracy)
print()
print(cm)
print()
print(cr)

"""#-[8]. Generating "reto3_Ypred.csv" with the model

### Loading the dataset to be predicted
"""

X_test_set = pd.read_csv('Datasets/reto3_testX.csv')

print("Loaded !")

"""### Scaling X_test_set dataset"""

X_test_set_scale = scaler.transform(X_test_set)

# Verify scale is good

fig, axes = plt.subplots(1,4)
axes[0].scatter(X_test_set['altitud'], X_test_set['azimut'], X_test_set['inclinacion'], X_test_set['DH_agua'])
axes[0].set_title("Original 1")
axes[1].scatter(X_test_set['DV_agua'], X_test_set['sombra_3pm'], X_test_set['Reserva_4'], X_test_set['reserva_2'])
axes[1].set_title("Original 2")
axes[2].scatter(X_test_set_scale[:, 0], X_test_set_scale[:, 1], X_test_set_scale[:, 2], X_test_set_scale[:, 3])
axes[2].set_title("MinMax 1")
axes[3].scatter(X_test_set_scale[:, 4], X_test_set_scale[:, 8], X_test_set_scale[:, 13], X_test_set_scale[:, 11])
axes[3].set_title("MinMax 2")
print("Checking if the scaling as been well done")
plt.show()

"""### Predict the values"""

Y_pred_final = gb_clf.predict(X_test_set_scale)

"""### Generating the reto3_Ypred.csv file"""

Y_pred_final = np.array(Y_pred_final)
# --- Save prediction in .csv file.
np.savetxt('Datasets/reto3_Ypred.csv', Y_pred_final, fmt='%i', delimiter=',')

"""#-[9]. Saving model with pickle"""

import pickle
from datetime import datetime

now = datetime.now()
date = now.strftime("%d-%m-%H:%M:%S")

filename = "Model_"
path_file = filename + date + ".sav"

pickle.dump(gb_clf, open(path_file, "wb"))
